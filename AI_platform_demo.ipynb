{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esIMGVxhDI0f"
   },
   "outputs": [],
   "source": [
    "#@title Copyright 2021 Google LLC. { display-mode: \"form\" }\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV1xZ1CPi3Nw"
   },
   "source": [
    "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
    "<a target=\"_blank\"  href=\"http://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/AI_platform_demo.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
    "</td><td>\n",
    "<a target=\"_blank\"  href=\"https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/AI_platform_demo.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SHAc5qbiR8l"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This is a demonstration notebook.  Suppose you have developed a model the training of which is constrained by the resources available to the notbook VM.  In that case, you may want to use the [Google AI Platform](https://cloud.google.com/ml-engine/docs/tensorflow/) to train your model.  The advantage of that is that long-running or resource intensive training jobs can be performed in the background.  Also, to use your trained model in Earth Engine, it needs to be [deployed as a hosted model](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models) on AI Platform.  This notebook uses previously created training data (see [this example notebook](https://colab.sandbox.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb)) and AI Platform to train a model, deploy it and use it to make predictions in Earth Engine.  To do that, code [needs to be structured as a python package](https://cloud.google.com/ml-engine/docs/tensorflow/packaging-trainer) that can be uploaded to AI Platform.  The following cells produce that package programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MJ4kW1pEhwP"
   },
   "source": [
    "# Setup software libraries\n",
    "\n",
    "Install needed libraries to the notebook VM.  Authenticate as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages (0.15.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages (from tensorflow-addons) (2.13.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "neIa46CpciXq"
   },
   "outputs": [],
   "source": [
    "# # Cloud authentication.\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "import ee\n",
    "service_account = '171083136856-compute@developer.gserviceaccount.com'\n",
    "credentials = ee.ServiceAccountCredentials(service_account, 'privatekey.json')\n",
    "ee.Initialize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jat01FEoUMqg"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=5zaOGdccmHsTlcfhY1ppTQJnDGayIIw8DikoZYAIdxI&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=5zaOGdccmHsTlcfhY1ppTQJnDGayIIw8DikoZYAIdxI&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AX4XfWjy2XvG5hkMKPREMJ7fPF8WSfW9ZdLcfByAgvI28Em_-HUqx6J7hOg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "# Import and initialize the Earth Engine library.\n",
    "# import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "8RnZzcYhcpsQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow setup.\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "# print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "n1hFdpBQfyhN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.1.post1\n"
     ]
    }
   ],
   "source": [
    "# Folium setup.\n",
    "import folium\n",
    "print(folium.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO1apj_B4c2R"
   },
   "source": [
    "# Training code package setup\n",
    "\n",
    "It's necessary to create a Python package to hold the training code.  Here we're going to get started with that by creating a folder for the package and adding an empty `__init__.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WcO4hFne4yQ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1688\n",
      "-rw-r--r-- 1 gang_hong gang_hong  75971 Jan 16 16:10  AI_platform_demo.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong  21133 Jan 13 21:12  ALR_functions.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong 118952 Jan 14 19:24  Charlottetown_LAI_lars.png\n",
      "-rw-r--r-- 1 gang_hong gang_hong 157464 Jan 15 02:33  Charlottetown_LAI_rf_cart_comparison.png\n",
      "-rw-r--r-- 1 gang_hong gang_hong  44590 Jan 14 14:29 'Copy of Earth_Engine_TensorFlow_AI_Platform.ipynb'\n",
      "-rw-r--r-- 1 gang_hong gang_hong  47020 Jan 16 15:48 'Copy of Earth_Engine_TensorFlow_logistic_regression.ipynb'\n",
      "-rw-r--r-- 1 gang_hong gang_hong  42133 Jan 16 15:48  Earth_Engine_TensorFlow_logistic_regression.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong  46611 Jan 13 21:59  Earth_Engine_TensorFlow_logistic_regression_jupyter.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong 127348 Jan 13 21:12  Rice_Mapping_with_DNN.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong  17883 Jan 13 21:12  Untitled.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong   3672 Jan 13 21:12  Untitled1.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong  22328 Jan 13 21:12  Untitled2.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong     72 Jan 13 22:11  Untitled3.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong  10741 Jan 14 19:06  Untitled4.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong    604 Jan 14 19:58  Untitled5.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong     72 Jan 14 20:25  Untitled6.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong    631 Jan 15 02:34  Untitled7.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong   1154 Jan 15 02:36  Untitled8.ipynb\n",
      "drwxr-xr-x 2 gang_hong gang_hong   4096 Jan 13 21:45  __pycache__\n",
      "drwxr-xr-x 3 gang_hong gang_hong   4096 Jan 15 02:40  ai_platform_demo\n",
      "-rw-r--r-- 1 gang_hong gang_hong   6452 Jan 13 21:12  ee_functions.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong   3358 Jan 13 21:12  feature_collections.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong   3067 Jan 13 21:12  image_bands.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong   1832 Jan 14 19:17  nnet.csv\n",
      "-rw-r--r-- 1 gang_hong gang_hong   2320 Jan 14 01:20  privatekey.json\n",
      "-rw-r--r-- 1 gang_hong gang_hong   2320 Jan 13 21:12  privatekey0.json\n",
      "-rw-r--r-- 1 gang_hong gang_hong 319455 Jan 13 21:12  regression_approaches.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong 100179 Jan 14 20:02  regression_approaches_VM_version.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong 473644 Jan 13 21:03  vm-20220113T210056Z-001.zip\n",
      "-rw-r--r-- 1 gang_hong gang_hong   8031 Jan 13 21:12  wrapper_nets.py\n",
      "mkdir: cannot create directory ‘ai_platform_demo’: File exists\n",
      "total 16\n",
      "-rw-r--r-- 1 gang_hong gang_hong    0 Jan 16 16:11 __init__.py\n",
      "drwxr-xr-x 2 gang_hong gang_hong 4096 Jan 16 15:41 __pycache__\n",
      "-rw-r--r-- 1 gang_hong gang_hong 1395 Jan 16 15:47 config.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong 3634 Jan 16 15:47 model.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong  519 Jan 16 00:07 task.py\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PATH = 'ai_platform_demo'\n",
    "\n",
    "!ls -l\n",
    "!mkdir {PACKAGE_PATH}\n",
    "!touch {PACKAGE_PATH}/__init__.py\n",
    "!ls -l {PACKAGE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT8ycmzClYwf"
   },
   "source": [
    "## Variables\n",
    "\n",
    "These variables need to be stored in a place where other code can access them.  There are a variety of ways of accomplishing that, but here we'll use the `%%writefile` command to write the contents of the code cell to a file called `config.py`.\n",
    "\n",
    "**Note:** You need to insert the name of a bucket (below) to which you have write access!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "psz7wJKalaoj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ai_platform_demo/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PACKAGE_PATH}/config.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# INSERT YOUR PROJECT HERE!\n",
    "PROJECT = 'ccmeo-ag-000008'\n",
    "\n",
    "# INSERT YOUR BUCKET HERE!\n",
    "BUCKET = 'eealr'\n",
    "\n",
    "# This is a good region for hosting AI models.\n",
    "REGION = 'northamerica-northeast1'\n",
    "\n",
    "# Specify names of output locations in Cloud Storage.\n",
    "FOLDER = 'fcnn-demo'\n",
    "JOB_DIR = 'gs://' + BUCKET + '/' + FOLDER + '/trainer'\n",
    "MODEL_DIR = JOB_DIR + '/model'\n",
    "LOGS_DIR = JOB_DIR + '/logs'\n",
    "\n",
    "# Put the EEified model next to the trained model directory.\n",
    "EEIFIED_DIR = JOB_DIR + '/eeified'\n",
    "\n",
    "# Pre-computed training and eval data.\n",
    "DATA_BUCKET = 'ee-docs-demos'\n",
    "TRAINING_BASE = 'training_patches'\n",
    "EVAL_BASE = 'eval_patches'\n",
    "\n",
    "# Specify inputs (Landsat bands) to the model and the response variable.\n",
    "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "thermalBands = ['B10', 'B11']\n",
    "BANDS = opticalBands + thermalBands\n",
    "RESPONSE = 'impervious'\n",
    "FEATURES = BANDS + [RESPONSE]\n",
    "\n",
    "# Specify the size and shape of patches expected by the model.\n",
    "KERNEL_SIZE = 256\n",
    "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
    "COLUMNS = [\n",
    "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
    "]\n",
    "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
    "\n",
    "# Sizes of the training and evaluation datasets.\n",
    "TRAIN_SIZE = 16000\n",
    "EVAL_SIZE = 8000\n",
    "\n",
    "# Specify model training parameters.\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "# EPOCHS = 50\n",
    "BUFFER_SIZE = 3000\n",
    "OPTIMIZER = 'SGD'\n",
    "LOSS = 'MeanSquaredError'\n",
    "METRICS = ['RootMeanSquaredError']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0feVjClV6dxz"
   },
   "source": [
    "Verify that the written file has the expected contents and is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6_BEz5Zn6LvT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "# INSERT YOUR PROJECT HERE!\n",
      "PROJECT = 'ccmeo-ag-000008'\n",
      "\n",
      "# INSERT YOUR BUCKET HERE!\n",
      "BUCKET = 'eealr'\n",
      "\n",
      "# This is a good region for hosting AI models.\n",
      "REGION = 'northamerica-northeast1'\n",
      "\n",
      "# Specify names of output locations in Cloud Storage.\n",
      "FOLDER = 'fcnn-demo'\n",
      "JOB_DIR = 'gs://' + BUCKET + '/' + FOLDER + '/trainer'\n",
      "MODEL_DIR = JOB_DIR + '/model'\n",
      "LOGS_DIR = JOB_DIR + '/logs'\n",
      "\n",
      "# Put the EEified model next to the trained model directory.\n",
      "EEIFIED_DIR = JOB_DIR + '/eeified'\n",
      "\n",
      "# Pre-computed training and eval data.\n",
      "DATA_BUCKET = 'ee-docs-demos'\n",
      "TRAINING_BASE = 'training_patches'\n",
      "EVAL_BASE = 'eval_patches'\n",
      "\n",
      "# Specify inputs (Landsat bands) to the model and the response variable.\n",
      "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
      "thermalBands = ['B10', 'B11']\n",
      "BANDS = opticalBands + thermalBands\n",
      "RESPONSE = 'impervious'\n",
      "FEATURES = BANDS + [RESPONSE]\n",
      "\n",
      "# Specify the size and shape of patches expected by the model.\n",
      "KERNEL_SIZE = 256\n",
      "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
      "COLUMNS = [\n",
      "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
      "]\n",
      "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
      "\n",
      "# Sizes of the training and evaluation datasets.\n",
      "TRAIN_SIZE = 16000\n",
      "EVAL_SIZE = 8000\n",
      "\n",
      "# Specify model training parameters.\n",
      "BATCH_SIZE = 16\n",
      "EPOCHS = 10\n",
      "# EPOCHS = 50\n",
      "BUFFER_SIZE = 3000\n",
      "OPTIMIZER = 'SGD'\n",
      "LOSS = 'MeanSquaredError'\n",
      "METRICS = ['RootMeanSquaredError']\n",
      "\n",
      "\n",
      " 16\n"
     ]
    }
   ],
   "source": [
    "!cat {PACKAGE_PATH}/config.py\n",
    "\n",
    "from ai_platform_demo import config\n",
    "print('\\n\\n', config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgoDc7Hilfc4"
   },
   "source": [
    "## Training data, evaluation data and model\n",
    "\n",
    "The following is code to load training/evaluation data and the model.  Write this into `model.py`. Note that these functions are developed and explained in [this example notebook](https://colab.sandbox.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "beiasALl-GPo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ai_platform_demo/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PACKAGE_PATH}/model.py\n",
    "\n",
    "from . import config\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import metrics\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import optimizers\n",
    "\n",
    "# Dataset loading functions\n",
    "\n",
    "def parse_tfrecord(example_proto):\n",
    "  return tf.io.parse_single_example(example_proto, config.FEATURES_DICT)\n",
    "\n",
    "def to_tuple(inputs):\n",
    "  inputsList = [inputs.get(key) for key in config.FEATURES]\n",
    "  stacked = tf.stack(inputsList, axis=0)\n",
    "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "  return stacked[:,:,:len(config.BANDS)], stacked[:,:,len(config.BANDS):]\n",
    "\n",
    "def get_dataset(pattern):\n",
    "\tglob = tf.io.gfile.glob(pattern)\n",
    "\tdataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "\tdataset = dataset.map(parse_tfrecord)\n",
    "\tdataset = dataset.map(to_tuple)\n",
    "\treturn dataset\n",
    "\n",
    "def get_training_dataset():\n",
    "\tglob = 'gs://' + config.DATA_BUCKET + '/' + config.FOLDER + '/' + config.TRAINING_BASE + '*'\n",
    "\tdataset = get_dataset(glob)\n",
    "\tdataset = dataset.shuffle(config.BUFFER_SIZE).batch(config.BATCH_SIZE).repeat()\n",
    "\treturn dataset\n",
    "\n",
    "def get_eval_dataset():\n",
    "\tglob = 'gs://' + config.DATA_BUCKET + '/' + config.FOLDER + '/' + config.EVAL_BASE + '*'\n",
    "\tdataset = get_dataset(glob)\n",
    "\tdataset = dataset.batch(1).repeat()\n",
    "\treturn dataset\n",
    "\n",
    "# A variant of the UNET model.\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "\tencoder = layers.BatchNormalization()(encoder)\n",
    "\tencoder = layers.Activation('relu')(encoder)\n",
    "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "\tencoder = layers.BatchNormalization()(encoder)\n",
    "\tencoder = layers.Activation('relu')(encoder)\n",
    "\treturn encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "\tencoder = conv_block(input_tensor, num_filters)\n",
    "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "\treturn encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\treturn decoder\n",
    "\n",
    "def get_model():\n",
    "\tinputs = layers.Input(shape=[None, None, len(config.BANDS)]) # 256\n",
    "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
    "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
    "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
    "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
    "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
    "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
    "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
    "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
    "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
    "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
    "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
    "\toutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
    "\n",
    "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "\tmodel.compile(\n",
    "\t\toptimizer=optimizers.get(config.OPTIMIZER), \n",
    "\t\tloss=losses.get(config.LOSS),\n",
    "\t\tmetrics=[metrics.get(metric) for metric in config.METRICS])\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0F5czqrABgk"
   },
   "source": [
    "Verify that `model.py` is functioning as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "8b0I9BaJ-GXw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 256, 256, 9), dtype=float32, numpy=\n",
      "array([[[[0.0276 , 0.0326 , 0.0582 , ..., 0.1083 , 0.2035 , 0.1895 ],\n",
      "         [0.0276 , 0.0326 , 0.0582 , ..., 0.1083 , 0.2035 , 0.1895 ],\n",
      "         [0.0314 , 0.0368 , 0.0679 , ..., 0.1172 , 0.2065 , 0.1905 ],\n",
      "         ...,\n",
      "         [0.0159 , 0.01885, 0.0391 , ..., 0.06775, 0.1965 , 0.1775 ],\n",
      "         [0.0159 , 0.01885, 0.0391 , ..., 0.06775, 0.1965 , 0.1775 ],\n",
      "         [0.01445, 0.01785, 0.0361 , ..., 0.0628 , 0.1965 , 0.1775 ]],\n",
      "\n",
      "        [[0.0263 , 0.0296 , 0.0537 , ..., 0.0917 , 0.2055 , 0.1885 ],\n",
      "         [0.0263 , 0.0296 , 0.0537 , ..., 0.0917 , 0.2055 , 0.1885 ],\n",
      "         [0.02835, 0.0326 , 0.0606 , ..., 0.10645, 0.209  , 0.1945 ],\n",
      "         ...,\n",
      "         [0.01605, 0.01965, 0.04185, ..., 0.0653 , 0.2015 , 0.1775 ],\n",
      "         [0.01605, 0.01965, 0.04185, ..., 0.0653 , 0.2015 , 0.1775 ],\n",
      "         [0.01585, 0.02025, 0.04225, ..., 0.0657 , 0.203  , 0.1815 ]],\n",
      "\n",
      "        [[0.0262 , 0.0311 , 0.0536 , ..., 0.0861 , 0.2085 , 0.1945 ],\n",
      "         [0.0262 , 0.0311 , 0.0536 , ..., 0.0861 , 0.2085 , 0.1945 ],\n",
      "         [0.0265 , 0.0296 , 0.0559 , ..., 0.094  , 0.2095 , 0.2005 ],\n",
      "         ...,\n",
      "         [0.0162 , 0.0206 , 0.0465 , ..., 0.06855, 0.2085 , 0.184  ],\n",
      "         [0.0162 , 0.0206 , 0.0465 , ..., 0.06855, 0.2085 , 0.184  ],\n",
      "         [0.0197 , 0.0256 , 0.0511 , ..., 0.08045, 0.21   , 0.1895 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157 , 0.0201 , 0.0379 , ..., 0.0509 , 0.1865 , 0.1725 ],\n",
      "         [0.02195, 0.02705, 0.0524 , ..., 0.0754 , 0.1965 , 0.179  ],\n",
      "         [0.02195, 0.02705, 0.0524 , ..., 0.0754 , 0.1965 , 0.179  ],\n",
      "         ...,\n",
      "         [0.0105 , 0.0132 , 0.0272 , ..., 0.0314 , 0.1955 , 0.1845 ],\n",
      "         [0.0105 , 0.01335, 0.02925, ..., 0.0346 , 0.195  , 0.1805 ],\n",
      "         [0.0105 , 0.01335, 0.02925, ..., 0.0346 , 0.195  , 0.1805 ]],\n",
      "\n",
      "        [[0.0173 , 0.0203 , 0.0401 , ..., 0.0545 , 0.1875 , 0.1735 ],\n",
      "         [0.02125, 0.02735, 0.05265, ..., 0.08935, 0.1965 , 0.1785 ],\n",
      "         [0.02125, 0.02735, 0.05265, ..., 0.08935, 0.1965 , 0.1785 ],\n",
      "         ...,\n",
      "         [0.0101 , 0.0123 , 0.0267 , ..., 0.0355 , 0.1965 , 0.1785 ],\n",
      "         [0.0099 , 0.0139 , 0.0303 , ..., 0.0366 , 0.1965 , 0.1785 ],\n",
      "         [0.0099 , 0.0139 , 0.0303 , ..., 0.0366 , 0.1965 , 0.1785 ]],\n",
      "\n",
      "        [[0.01655, 0.0197 , 0.03715, ..., 0.05075, 0.1875 , 0.172  ],\n",
      "         [0.01965, 0.02395, 0.0487 , ..., 0.07765, 0.196  , 0.178  ],\n",
      "         [0.01965, 0.02395, 0.0487 , ..., 0.07765, 0.196  , 0.178  ],\n",
      "         ...,\n",
      "         [0.011  , 0.01355, 0.0287 , ..., 0.0433 , 0.1985 , 0.1775 ],\n",
      "         [0.0107 , 0.0144 , 0.0265 , ..., 0.0398 , 0.1975 , 0.1795 ],\n",
      "         [0.0107 , 0.0144 , 0.0265 , ..., 0.0398 , 0.1975 , 0.1795 ]]]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 256, 256, 1), dtype=float32, numpy=\n",
      "array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]], dtype=float32)>)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 3 2624        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, None, 3 128         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, None, None, 3 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 3 9248        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, None, 3 128         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, None, None, 3 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, None, None, 3 0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 6 18496       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, None, 6 256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, None, None, 6 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 6 36928       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, None, 6 256         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, None, 6 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, None, None, 6 0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 1 73856       max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, None, 1 512         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, None, 1 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 1 147584      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, None, 1 512         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, None, None, 1 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, None, None, 1 0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 2 295168      max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 2 1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, None, None, 2 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 2 590080      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 2 1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, None, None, 2 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, None, None, 2 0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 5 1180160     max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, None, 5 2048        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, None, None, 5 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 5 2359808     activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, None, 5 2048        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, None, 5 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, None, None, 5 0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 1 4719616     max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, None, 1 4096        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, None, 1 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 1 9438208     activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, None, 1 4096        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, None, 1 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DTran (None, None, None, 5 2097664     activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, None, 1 0           activation_63[0][0]              \n",
      "                                                                 conv2d_transpose_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, None, None, 1 4096        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, None, None, 1 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 5 4719104     activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, None, 5 2048        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, None, None, 5 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 5 2359808     activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, None, 5 2048        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, None, None, 5 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, None, None, 2 524544      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, None, None, 5 0           activation_61[0][0]              \n",
      "                                                                 conv2d_transpose_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, None, 5 2048        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, None, None, 5 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, None, None, 2 1179904     activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, None, 2 1024        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, None, None, 2 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, None, None, 2 590080      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, None, 2 1024        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, None, None, 2 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DTran (None, None, None, 1 131200      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, None, None, 2 0           activation_59[0][0]              \n",
      "                                                                 conv2d_transpose_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, None, None, 2 1024        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, None, None, 2 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, None, None, 1 295040      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, None, None, 1 512         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, None, None, 1 147584      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, None, None, 1 512         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, None, None, 1 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DTran (None, None, None, 6 32832       activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, None, None, 1 0           activation_57[0][0]              \n",
      "                                                                 conv2d_transpose_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, None, None, 1 512         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, None, None, 1 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, None, None, 6 73792       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, None, None, 6 256         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, None, None, 6 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, None, None, 6 36928       activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, None, None, 6 256         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, None, None, 6 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DTran (None, None, None, 3 8224        activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, None, None, 6 0           activation_55[0][0]              \n",
      "                                                                 conv2d_transpose_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, None, None, 6 256         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, None, None, 6 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, None, None, 3 18464       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, None, None, 3 128         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, None, None, 3 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, None, None, 3 9248        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, None, None, 3 128         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, None, None, 3 0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, None, None, 1 33          activation_80[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 31,128,225\n",
      "Trainable params: 31,112,225\n",
      "Non-trainable params: 16,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from ai_platform_demo import model\n",
    "\n",
    "eval = model.get_eval_dataset()\n",
    "print(iter(eval.take(1)).next())\n",
    "\n",
    "model = model.get_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lul-C8DBXHT"
   },
   "source": [
    "## Training task\n",
    "\n",
    "At this stage, there should be `config.py` storing variables and `model.py` which has code for getting the training/evaluation data and the model.  All that's left is code for training the model.  The following will create `task.py`, which will get the training and eval data, train the model and save it when it's done in a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "aR8GrYZd-Gb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ai_platform_demo/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PACKAGE_PATH}/task.py\n",
    "\n",
    "from . import config\n",
    "from . import model\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  training = model.get_training_dataset()\n",
    "  evaluation = model.get_eval_dataset()\n",
    "\n",
    "  m = model.get_model()\n",
    "\n",
    "  m.fit(\n",
    "      x=training,\n",
    "      epochs=config.EPOCHS, \n",
    "      steps_per_epoch=int(config.TRAIN_SIZE / config.BATCH_SIZE), \n",
    "      validation_data=evaluation,\n",
    "      validation_steps=int(config.EVAL_SIZE),\n",
    "      callbacks=[tf.keras.callbacks.TensorBoard(config.LOGS_DIR)])\n",
    "\n",
    "  m.save(config.MODEL_DIR, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTYQ8ftjCqgP"
   },
   "source": [
    "# Submit the package to AI Platform for training\n",
    "\n",
    "Now there's everything to submit this job, which can be done from the command line.  First, define some needed variables.\n",
    "\n",
    "**Note:** You need to insert the name of a Cloud project (below) you own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "p-PtuGdnEGcv"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "JOB_NAME = 'demo_training_job_' + str(int(time.time()))\n",
    "TRAINER_PACKAGE_PATH = 'ai_platform_demo'\n",
    "MAIN_TRAINER_MODULE = 'ai_platform_demo.task'\n",
    "REGION = 'northamerica-northeast1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVXri8QJETBb"
   },
   "source": [
    "Now the training job is ready to be started.  First, you need to enable the ML API for your project.  This can be done from [this link to the Cloud Console](https://console.developers.google.com/apis/library/ml.googleapis.com).  See [this guide](https://cloud.google.com/ml-engine/docs/tensorflow/training-jobs) for details.  Note that the Python and Tensorflow versions should match what is used in the Colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "B_sQ1mo6-Gef"
   },
   "outputs": [],
   "source": [
    "# !gcloud ai-platform jobs submit training {JOB_NAME} \\\n",
    "#     --job-dir {config.JOB_DIR}  \\\n",
    "#     --package-path {TRAINER_PACKAGE_PATH} \\\n",
    "#     --module-name {MAIN_TRAINER_MODULE} \\\n",
    "#     --region {REGION} \\\n",
    "#     --project {config.PROJECT} \\\n",
    "#     --runtime-version 2.7\\\n",
    "#     --python-version 3.7 \\\n",
    "#     --scale-tier custom \\\n",
    "#     --master-machine-type n1-highcpu-32 \n",
    "#     # --service-account '171083136856-compute@developer.gserviceaccount.com'\n",
    "#     # --scale-tier basic-gpu\n",
    "#     # --service-account '171083136856@cloudservices.gserviceaccount.com'\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [demo_training_job_1642349551] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe demo_training_job_1642349551\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs demo_training_job_1642349551\n",
      "jobId: demo_training_job_1642349551\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training {JOB_NAME} \\\n",
    "    --job-dir {config.JOB_DIR}  \\\n",
    "    --package-path {TRAINER_PACKAGE_PATH} \\\n",
    "    --module-name {MAIN_TRAINER_MODULE} \\\n",
    "    --region {REGION} \\\n",
    "    --project {config.PROJECT} \\\n",
    "    --runtime-version 2.7\\\n",
    "    --python-version 3.7 \\\n",
    "    --scale-tier custom \\\n",
    "    --master-machine-type n1-highmem-64 \\\n",
    "    --master-accelerator count=4,type=NVIDIA-TESLA-P4 \n",
    "    # --service-account '171083136856-compute@developer.gserviceaccount.com'\n",
    "    # --scale-tier basic-gpu\n",
    "    # --service-account '171083136856@cloudservices.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for adding GPU\n",
    "# !gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "#         --package-path $APP_PACKAGE_PATH \\\n",
    "#         --module-name $MAIN_APP_MODULE \\\n",
    "#         --job-dir $JOB_DIR \\\n",
    "#         --region us-central1 \\\n",
    "#         --scale-tier custom \\\n",
    "#         --master-machine-type n1-highcpu-16 \\\n",
    "#         --master-accelerator count=4,type=nvidia-tesla-t4 \\\n",
    "#         --worker-count 9 \\\n",
    "#         --worker-machine-type n1-highcpu-16 \\\n",
    "#         --worker-accelerator count=4,type=nvidia-tesla-t4 \\\n",
    "#         --parameter-server-count 3 \\\n",
    "#         --parameter-server-machine-type n1-highmem-8 \\\n",
    "#         -- \\\n",
    "#         --user_arg_1 value_1 \\\n",
    "#          ...\n",
    "#         --user_arg_n value_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6R65k0viIJS"
   },
   "source": [
    "## Monitor the training job\n",
    "\n",
    "There's not much more to do until the model is finished training (~24 hours), but it's fun and useful to monitor its progress. You can do that programmatically with another `gcloud` command.  The output of that command can be read into an `IPython.utils.text.SList` from which the `state` is extracted and ensured to be `SUCCEEDED`.  Or you can monitor it from the [AI Platform jobs page](http://console.cloud.google.com/ai-platform/jobs) on the Cloud Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1oqR6sCrEGoB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"createTime: '2022-01-16T16:12:50Z'\", \"endTime: '2022-01-16T18:48:29Z'\", 'etag: VLlt3cVFz2I=', 'jobId: demo_training_job_1642349551', \"jobPosition: '0'\", \"startTime: '2022-01-16T16:25:45Z'\", 'state: SUCCEEDED', 'trainingInput:', '  jobDir: gs://eealr/fcnn-demo/trainer', '  masterConfig:', '    acceleratorConfig:', \"      count: '4'\", '      type: NVIDIA_TESLA_P4', '  masterType: n1-highmem-64', '  packageUris:', '  - gs://eealr/fcnn-demo/trainer/packages/1f9714c25ade927133b13bab33a98537e7636506b3b39c8ccbcb452cca5804aa/ai_platform_demo-0.0.0.tar.gz', '  pythonModule: ai_platform_demo.task', \"  pythonVersion: '3.7'\", '  region: northamerica-northeast1', \"  runtimeVersion: '2.7'\", '  scaleTier: CUSTOM', 'trainingOutput:', '  consumedMLUnits: 29.44', '', 'View job in the Cloud Console at:', 'https://console.cloud.google.com/mlengine/jobs/demo_training_job_1642349551?project=ccmeo-ag-000008', '', 'View logs at:', 'https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Fdemo_training_job_1642349551&project=ccmeo-ag-000008']\n",
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'ccmeo-ag-000008'\n",
    "# print (PROJECT)\n",
    "desc = !gcloud ai-platform jobs describe {JOB_NAME} --project {PROJECT}\n",
    "print (desc)\n",
    "state = desc.grep('state:')[0].split(':')[1].strip()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFnIrvO0StiO"
   },
   "source": [
    "# Inspect the trained model\n",
    "\n",
    "Once the training job has finished, verify that you can load the trained model and print a summary of the fitted parameters.  It's also useful to examine the logs with [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard).  There's a convenient notebook extension that will launch TensorBoard in the Colab notebook.  Examine the training and testing learning curves to ensure that the training process has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "T9GU8Pl-2Y5p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9276afc95d7a2874\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9276afc95d7a2874\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVRifmE2ffvv"
   },
   "source": [
    "# Prepare the model for making predictions in Earth Engine\n",
    "\n",
    "Before we can use the model in Earth Engine, it needs to be hosted by AI Platform.  But before we can host the model on AI Platform we need to *EEify* (a new word!) it.  The EEification process merely appends some extra operations to the input and outputs of the model in order to accommodate the interchange format between pixels from Earth Engine (float32) and inputs to AI Platform (base64).  (See [this doc](https://cloud.google.com/ml-engine/docs/online-predict#binary_data_in_prediction_input) for details.)  \n",
    "\n",
    "## `earthengine model prepare`\n",
    "The EEification process is handled for you using the Earth Engine command `earthengine model prepare`.  To use that command, we need to specify the input and output model directories and the name of the input and output nodes in the TensorFlow computation graph.  We can do all that programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "KTzneSE_2WgL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'{\"serving_default_input_1:0\": \"array\"}'\n",
      "'{\"StatefulPartitionedCall:0\": \"impervious\"}'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "meta_graph_def = saved_model_utils.get_meta_graph_def(config.MODEL_DIR, 'serve')\n",
    "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
    "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
    "\n",
    "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
    "# model only has a single input and a single output.\n",
    "input_name = None\n",
    "for k,v in inputs.items():\n",
    "  input_name = v.name\n",
    "  break\n",
    "\n",
    "output_name = None\n",
    "for k,v in outputs.items():\n",
    "  output_name = v.name\n",
    "  break\n",
    "\n",
    "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
    "# AI Platform inputs and outputs, respectively.\n",
    "import json\n",
    "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
    "output_dict = \"'\" + json.dumps({output_name: \"impervious\"}) + \"'\"\n",
    "print(input_dict)\n",
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "C1rA8oyscmGG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved project id\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1380, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1363, in _run_fn\n",
      "    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1456, in _call_tf_sessionrun\n",
      "    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) RESOURCE_EXHAUSTED: SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;0000000000000001;/job:localhost/replica:0/task:0/device:GPU:0;edge_694_StatefulPartitionedCall_2/RestoreV2;0:0\n",
      "\t [[{{function_node __inference__traced_restore_198904}}{{node RestoreV2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "\t [[GroupCrossDeviceControlEdges_0/StatefulPartitionedCall_2/Identity_172/_354]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "  (1) RESOURCE_EXHAUSTED: SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;0000000000000001;/job:localhost/replica:0/task:0/device:GPU:0;edge_694_StatefulPartitionedCall_2/RestoreV2;0:0\n",
      "\t [[{{function_node __inference__traced_restore_198904}}{{node RestoreV2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/bin/earthengine\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/eecli.py\", line 82, in main\n",
      "    tf_module.app.run(_run_command, argv=sys.argv[:1])\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/absl/app.py\", line 300, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/eecli.py\", line 63, in _run_command\n",
      "    dispatcher.run(args, config)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/commands.py\", line 360, in run\n",
      "    self.command_dict[vars(args)[self.dest]].run(args, config)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/commands.py\", line 360, in run\n",
      "    self.command_dict[vars(args)[self.dest]].run(args, config)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/commands.py\", line 1798, in run\n",
      "    new_model_dir = PrepareModelCommand._make_rpc_friendly(\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/commands.py\", line 1715, in _make_rpc_friendly\n",
      "    meta_graph = tf.saved_model.load(sesh, [tag], model_dir)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 348, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 346, in load\n",
      "    return loader.load(sess, tags, import_scope, **saver_kwargs)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 503, in load\n",
      "    self.restore_variables(sess, saver, import_scope)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 454, in restore_variables\n",
      "    saver.restore(sess, self._variables_path)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 1404, in restore\n",
      "    sess.run(self.saver_def.restore_op_name,\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 970, in run\n",
      "    result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1193, in _run\n",
      "    results = self._do_run(handle, final_targets, final_fetches,\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1373, in _do_run\n",
      "    return self._do_call(_run_fn, feeds, fetches, targets, options,\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1399, in _do_call\n",
      "    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) RESOURCE_EXHAUSTED: SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;0000000000000001;/job:localhost/replica:0/task:0/device:GPU:0;edge_694_StatefulPartitionedCall_2/RestoreV2;0:0\n",
      "\t [[{{node RestoreV2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "\t [[GroupCrossDeviceControlEdges_0/StatefulPartitionedCall_2/Identity_172/_354]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "  (1) RESOURCE_EXHAUSTED: SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;0000000000000001;/job:localhost/replica:0/task:0/device:GPU:0;edge_694_StatefulPartitionedCall_2/RestoreV2;0:0\n",
      "\t [[{{node RestoreV2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n"
     ]
    }
   ],
   "source": [
    "# You need to set the project before using the model prepare command.\n",
    "!earthengine set_project {PROJECT}\n",
    "!earthengine model prepare --source_dir {config.MODEL_DIR} --dest_dir {config.EEIFIED_DIR} --input {input_dict} --output {output_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buDXUtISnwm0"
   },
   "source": [
    "Note that you can also use the TensorFlow saved model command line tool to do this manually.  See [this doc](https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel) for details.  Also note the names we've specified for the new inputs and outputs: `array` and `impervious`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hno8QSo-2XjQ"
   },
   "source": [
    "# Perform inference using the trained model in Earth Engine\n",
    "\n",
    "Before it's possible to get predictions from the trained and EEified model, it needs to be deployed on AI Platform.  The first step is to create the model.  The second step is to create a version.  See [this guide](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models) for details.  Note that models and versions can be monitored from the [AI Platform models page](http://console.cloud.google.com/ai-platform/models) of the Cloud Console.\n",
    "\n",
    "To ensure that the model is ready for predictions without having to warm up nodes, you can use a configuration yaml file to set the scaling type of this version to `autoScaling`, and, set a minimum number of nodes for the version. This will ensure there are always nodes on stand-by, however, you will be charged as long as they are running. For this example, we'll set the `minNodes` to 10. That means that at a minimum, 10 nodes are always up and running and waiting for predictions. The number of nodes will also scale up automatically if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "6Wtt5NYZDjB7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "autoScaling:\n",
    "    minNodes: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "KSp34aCaySu5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating version: v1642359119\n",
      "Using endpoint [https://northamerica-northeast1-ml.googleapis.com/]\n",
      "Created ai platform model [projects/ccmeo-ag-000008/models/fcnn_demo_model].\n",
      "Using endpoint [https://northamerica-northeast1-ml.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.versions.create) FAILED_PRECONDITION: Field: version.deployment_uri Error: The provided URI for model files doesn't contain any objects.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: The provided URI for model files doesn't contain any objects.\n",
      "    field: version.deployment_uri\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'fcnn_demo_model'\n",
    "VERSION_NAME = 'v' + str(int(time.time()))\n",
    "print('Creating version: ' + VERSION_NAME)\n",
    "\n",
    "!gcloud ai-platform models create {MODEL_NAME} \\\n",
    "  --project {PROJECT} \\\n",
    "  --region {REGION}\n",
    "\n",
    "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
    "  --project {config.PROJECT} \\\n",
    "  --model {MODEL_NAME} \\\n",
    "  --region {REGION} \\\n",
    "  --origin {config.EEIFIED_DIR} \\\n",
    "  --framework \"TENSORFLOW\" \\\n",
    "  --runtime-version 2.4 \\\n",
    "  --python-version 3.7 \\\n",
    "  --config=config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4IfxvhmDWS"
   },
   "source": [
    "There is now a trained model, prepared for serving to Earth Engine, hosted and versioned on AI Platform.  We can now connect Earth Engine directly to the trained model for inference.  You do that with the `ee.Model.fromAiPlatformPredictor` command.\n",
    "\n",
    "## `ee.Model.fromAiPlatformPredictor`\n",
    "For this command to work, we need to know a lot about the model.  To connect to the model, you need to know the name and version.\n",
    "\n",
    "### Inputs\n",
    "You need to be able to recreate the imagery on which it was trained in order to perform inference.  Specifically, you need to create an array-valued input from the scaled data and use that for input.  (Recall that the new input node is named `array`, which is convenient because the array image has one band, named `array` by default.)  The inputs will be provided as 144x144 patches (`inputTileSize`), at 30-meter resolution (`proj`), but 8 pixels will be thrown out (`inputOverlapSize`) to minimize boundary effects.\n",
    "\n",
    "### Outputs\n",
    "The output (which you also need to know), is a single float band named `impervious`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqcgSGvx-E94"
   },
   "outputs": [],
   "source": [
    "# Use Landsat 8 surface reflectance data.\n",
    "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "\n",
    "# Cloud masking function.\n",
    "def maskL8sr(image):\n",
    "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
    "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
    "  qa = image.select('pixel_qa')\n",
    "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
    "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
    "  mask2 = image.mask().reduce('min')\n",
    "  mask3 = image.select(config.opticalBands).gt(0).And(\n",
    "          image.select(config.opticalBands).lt(10000)).reduce('min')\n",
    "  mask = mask1.And(mask2).And(mask3)\n",
    "  return image.select(config.opticalBands).divide(10000).addBands(\n",
    "          image.select(config.thermalBands).divide(10).clamp(273.15, 373.15)\n",
    "            .subtract(273.15).divide(100)).updateMask(mask)\n",
    "\n",
    "# The image input data is a cloud-masked median composite.\n",
    "image = l8sr.filterDate(\n",
    "    '2015-01-01', '2017-12-31').map(maskL8sr).median().select(config.BANDS).float()\n",
    "\n",
    "# Load the trained model and use it for prediction.  If you specified a region \n",
    "# other than the default (us-central1) at model creation, specify it here.\n",
    "model = ee.Model.fromAiPlatformPredictor(\n",
    "    projectName = config.PROJECT,\n",
    "    modelName = MODEL_NAME,\n",
    "    version = VERSION_NAME,\n",
    "    inputTileSize = [144, 144],\n",
    "    inputOverlapSize = [8, 8],\n",
    "    proj = ee.Projection('EPSG:4326').atScale(30),\n",
    "    fixInputProj = True,\n",
    "    outputBands = {'impervious': {\n",
    "        'type': ee.PixelType.float()\n",
    "      }\n",
    "    }\n",
    ")\n",
    "predictions = model.predictImage(image.toArray())\n",
    "\n",
    "# Use folium to visualize the input imagery and the predictions.\n",
    "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
    "map = folium.Map(location=[38., -122.5], zoom_start=13)\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(map)\n",
    "\n",
    "mapid = predictions.getMapId({'min': 0, 'max': 1})\n",
    "\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='predictions',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI_platform_demo.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:leaf] *",
   "language": "python",
   "name": "conda-env-leaf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
