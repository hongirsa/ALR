{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esIMGVxhDI0f"
   },
   "outputs": [],
   "source": [
    "#@title Copyright 2021 Google LLC. { display-mode: \"form\" }\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV1xZ1CPi3Nw"
   },
   "source": [
    "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
    "<a target=\"_blank\"  href=\"http://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/AI_platform_demo.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
    "</td><td>\n",
    "<a target=\"_blank\"  href=\"https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/AI_platform_demo.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SHAc5qbiR8l"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This is a demonstration notebook.  Suppose you have developed a model the training of which is constrained by the resources available to the notbook VM.  In that case, you may want to use the [Google AI Platform](https://cloud.google.com/ml-engine/docs/tensorflow/) to train your model.  The advantage of that is that long-running or resource intensive training jobs can be performed in the background.  Also, to use your trained model in Earth Engine, it needs to be [deployed as a hosted model](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models) on AI Platform.  This notebook uses previously created training data (see [this example notebook](https://colab.sandbox.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb)) and AI Platform to train a model, deploy it and use it to make predictions in Earth Engine.  To do that, code [needs to be structured as a python package](https://cloud.google.com/ml-engine/docs/tensorflow/packaging-trainer) that can be uploaded to AI Platform.  The following cells produce that package programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MJ4kW1pEhwP"
   },
   "source": [
    "# Setup software libraries\n",
    "\n",
    "Install needed libraries to the notebook VM.  Authenticate as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages (0.15.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages (from tensorflow-addons) (2.13.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "neIa46CpciXq"
   },
   "outputs": [],
   "source": [
    "# # Cloud authentication.\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "import ee\n",
    "service_account = '171083136856-compute@developer.gserviceaccount.com'\n",
    "credentials = ee.ServiceAccountCredentials(service_account, 'privatekey.json')\n",
    "ee.Initialize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jat01FEoUMqg"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=Ktu0C5dZACz86zyPrpbqVhnCeHe1rkb7vaGwghaA5B8&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=Ktu0C5dZACz86zyPrpbqVhnCeHe1rkb7vaGwghaA5B8&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AX4XfWj50NXRllxB6j4QQ2oJ83Z03Iw6zz6mJmE7Iqxa2xTmWD9fXmf5Y2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "# Import and initialize the Earth Engine library.\n",
    "# import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8RnZzcYhcpsQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow setup.\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "# print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n1hFdpBQfyhN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.1.post1\n"
     ]
    }
   ],
   "source": [
    "# Folium setup.\n",
    "import folium\n",
    "print(folium.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO1apj_B4c2R"
   },
   "source": [
    "# Training code package setup\n",
    "\n",
    "It's necessary to create a Python package to hold the training code.  Here we're going to get started with that by creating a folder for the package and adding an empty `__init__.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WcO4hFne4yQ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1696\n",
      "-rw-r--r-- 1 gang_hong gang_hong  78311 Jan 17 19:05  AI_platform_demo.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong  21133 Jan 13 21:12  ALR_functions.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong 118952 Jan 14 19:24  Charlottetown_LAI_lars.png\n",
      "-rw-r--r-- 1 gang_hong gang_hong 157464 Jan 15 02:33  Charlottetown_LAI_rf_cart_comparison.png\n",
      "-rw-r--r-- 1 gang_hong gang_hong  44590 Jan 14 14:29 'Copy of Earth_Engine_TensorFlow_AI_Platform.ipynb'\n",
      "-rw-r--r-- 1 gang_hong gang_hong  47020 Jan 16 15:48 'Copy of Earth_Engine_TensorFlow_logistic_regression.ipynb'\n",
      "-rw-r--r-- 1 gang_hong gang_hong  42133 Jan 16 15:48  Earth_Engine_TensorFlow_logistic_regression.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong  46611 Jan 13 21:59  Earth_Engine_TensorFlow_logistic_regression_jupyter.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong 127348 Jan 13 21:12  Rice_Mapping_with_DNN.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong  17883 Jan 13 21:12  Untitled.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong   3672 Jan 13 21:12  Untitled1.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong  22328 Jan 13 21:12  Untitled2.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong     72 Jan 13 22:11  Untitled3.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong  10741 Jan 14 19:06  Untitled4.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong    604 Jan 14 19:58  Untitled5.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong     72 Jan 14 20:25  Untitled6.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong    631 Jan 15 02:34  Untitled7.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong   1154 Jan 15 02:36  Untitled8.ipynb\n",
      "drwxr-xr-x 2 gang_hong gang_hong   4096 Jan 13 21:45  __pycache__\n",
      "drwxr-xr-x 3 gang_hong gang_hong   4096 Jan 15 02:40  ai_platform_demo\n",
      "-rw-r--r-- 1 gang_hong gang_hong     30 Jan 16 18:51  config.yaml\n",
      "-rw-r--r-- 1 gang_hong gang_hong   6452 Jan 13 21:12  ee_functions.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong   3358 Jan 13 21:12  feature_collections.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong   3067 Jan 13 21:12  image_bands.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong   1832 Jan 14 19:17  nnet.csv\n",
      "-rw-r--r-- 1 gang_hong gang_hong   2320 Jan 14 01:20  privatekey.json\n",
      "-rw-r--r-- 1 gang_hong gang_hong   2320 Jan 13 21:12  privatekey0.json\n",
      "-rw-r--r-- 1 gang_hong gang_hong 319455 Jan 13 21:12  regression_approaches.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong 100179 Jan 14 20:02  regression_approaches_VM_version.ipynb\n",
      "-rw-r--r-- 1 gang_hong gang_hong 473644 Jan 13 21:03  vm-20220113T210056Z-001.zip\n",
      "-rw-r--r-- 1 gang_hong gang_hong   8031 Jan 13 21:12  wrapper_nets.py\n",
      "mkdir: cannot create directory ‘ai_platform_demo’: File exists\n",
      "total 16\n",
      "-rw-r--r-- 1 gang_hong gang_hong    0 Jan 17 19:05 __init__.py\n",
      "drwxr-xr-x 2 gang_hong gang_hong 4096 Jan 17 18:51 __pycache__\n",
      "-rw-r--r-- 1 gang_hong gang_hong 1409 Jan 17 18:51 config.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong 3634 Jan 17 18:51 model.py\n",
      "-rw-r--r-- 1 gang_hong gang_hong  519 Jan 17 18:51 task.py\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_PATH = 'ai_platform_demo'\n",
    "\n",
    "!ls -l\n",
    "!mkdir {PACKAGE_PATH}\n",
    "!touch {PACKAGE_PATH}/__init__.py\n",
    "!ls -l {PACKAGE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT8ycmzClYwf"
   },
   "source": [
    "## Variables\n",
    "\n",
    "These variables need to be stored in a place where other code can access them.  There are a variety of ways of accomplishing that, but here we'll use the `%%writefile` command to write the contents of the code cell to a file called `config.py`.\n",
    "\n",
    "**Note:** You need to insert the name of a bucket (below) to which you have write access!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "psz7wJKalaoj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ai_platform_demo/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PACKAGE_PATH}/config.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# INSERT YOUR PROJECT HERE!\n",
    "PROJECT = 'ccmeo-ag-000008'\n",
    "\n",
    "# INSERT YOUR BUCKET HERE!\n",
    "BUCKET = 'eealr'\n",
    "\n",
    "# This is a good region for hosting AI models.\n",
    "REGION = 'northamerica-northeast1'\n",
    "\n",
    "# Specify names of output locations in Cloud Storage.\n",
    "FOLDER = 'fcnn-demo'\n",
    "JOB_DIR = 'gs://' + BUCKET + '/' + FOLDER + '/trainer'\n",
    "MODEL_DIR = JOB_DIR + '/model'\n",
    "LOGS_DIR = JOB_DIR + '/logs'\n",
    "\n",
    "# Put the EEified model next to the trained model directory.\n",
    "EEIFIED_DIR = JOB_DIR + '/eeified'\n",
    "\n",
    "# Pre-computed training and eval data.\n",
    "DATA_BUCKET = 'ee-docs-demos'\n",
    "TRAINING_BASE = 'training_patches'\n",
    "EVAL_BASE = 'eval_patches'\n",
    "\n",
    "# Specify inputs (Landsat bands) to the model and the response variable.\n",
    "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "thermalBands = ['B10', 'B11']\n",
    "BANDS = opticalBands + thermalBands\n",
    "RESPONSE = 'impervious'\n",
    "FEATURES = BANDS + [RESPONSE]\n",
    "\n",
    "# Specify the size and shape of patches expected by the model.\n",
    "KERNEL_SIZE = 256\n",
    "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
    "COLUMNS = [\n",
    "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
    "]\n",
    "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
    "\n",
    "# Sizes of the training and evaluation datasets.\n",
    "TRAIN_SIZE = 16000\n",
    "EVAL_SIZE = 8000\n",
    "\n",
    "# Specify model training parameters.\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "# EPOCHS = 50\n",
    "BUFFER_SIZE = 3000\n",
    "OPTIMIZER = 'SGD'\n",
    "LOSS = 'MeanSquaredError'\n",
    "METRICS = ['RootMeanSquaredError']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0feVjClV6dxz"
   },
   "source": [
    "Verify that the written file has the expected contents and is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6_BEz5Zn6LvT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "# INSERT YOUR PROJECT HERE!\n",
      "PROJECT = 'ccmeo-ag-000008'\n",
      "\n",
      "# INSERT YOUR BUCKET HERE!\n",
      "BUCKET = 'eealr'\n",
      "\n",
      "# This is a good region for hosting AI models.\n",
      "REGION = 'northamerica-northeast1'\n",
      "\n",
      "# Specify names of output locations in Cloud Storage.\n",
      "FOLDER = 'fcnn-demo'\n",
      "JOB_DIR = 'gs://' + BUCKET + '/' + FOLDER + '/trainer'\n",
      "MODEL_DIR = JOB_DIR + '/model'\n",
      "LOGS_DIR = JOB_DIR + '/logs'\n",
      "\n",
      "# Put the EEified model next to the trained model directory.\n",
      "EEIFIED_DIR = JOB_DIR + '/eeified'\n",
      "\n",
      "# Pre-computed training and eval data.\n",
      "DATA_BUCKET = 'ee-docs-demos'\n",
      "TRAINING_BASE = 'training_patches'\n",
      "EVAL_BASE = 'eval_patches'\n",
      "\n",
      "# Specify inputs (Landsat bands) to the model and the response variable.\n",
      "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
      "thermalBands = ['B10', 'B11']\n",
      "BANDS = opticalBands + thermalBands\n",
      "RESPONSE = 'impervious'\n",
      "FEATURES = BANDS + [RESPONSE]\n",
      "\n",
      "# Specify the size and shape of patches expected by the model.\n",
      "KERNEL_SIZE = 256\n",
      "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
      "COLUMNS = [\n",
      "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
      "]\n",
      "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
      "\n",
      "# Sizes of the training and evaluation datasets.\n",
      "TRAIN_SIZE = 16000\n",
      "EVAL_SIZE = 8000\n",
      "\n",
      "# Specify model training parameters.\n",
      "BATCH_SIZE = 16\n",
      "EPOCHS = 5\n",
      "# EPOCHS = 50\n",
      "BUFFER_SIZE = 3000\n",
      "OPTIMIZER = 'SGD'\n",
      "LOSS = 'MeanSquaredError'\n",
      "METRICS = ['RootMeanSquaredError']\n",
      "\n",
      "\n",
      " 16\n"
     ]
    }
   ],
   "source": [
    "!cat {PACKAGE_PATH}/config.py\n",
    "\n",
    "from ai_platform_demo import config\n",
    "print('\\n\\n', config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgoDc7Hilfc4"
   },
   "source": [
    "## Training data, evaluation data and model\n",
    "\n",
    "The following is code to load training/evaluation data and the model.  Write this into `model.py`. Note that these functions are developed and explained in [this example notebook](https://colab.sandbox.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "beiasALl-GPo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ai_platform_demo/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PACKAGE_PATH}/model.py\n",
    "\n",
    "from . import config\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import metrics\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import optimizers\n",
    "\n",
    "# Dataset loading functions\n",
    "\n",
    "def parse_tfrecord(example_proto):\n",
    "  return tf.io.parse_single_example(example_proto, config.FEATURES_DICT)\n",
    "\n",
    "def to_tuple(inputs):\n",
    "  inputsList = [inputs.get(key) for key in config.FEATURES]\n",
    "  stacked = tf.stack(inputsList, axis=0)\n",
    "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "  return stacked[:,:,:len(config.BANDS)], stacked[:,:,len(config.BANDS):]\n",
    "\n",
    "def get_dataset(pattern):\n",
    "\tglob = tf.io.gfile.glob(pattern)\n",
    "\tdataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "\tdataset = dataset.map(parse_tfrecord)\n",
    "\tdataset = dataset.map(to_tuple)\n",
    "\treturn dataset\n",
    "\n",
    "def get_training_dataset():\n",
    "\tglob = 'gs://' + config.DATA_BUCKET + '/' + config.FOLDER + '/' + config.TRAINING_BASE + '*'\n",
    "\tdataset = get_dataset(glob)\n",
    "\tdataset = dataset.shuffle(config.BUFFER_SIZE).batch(config.BATCH_SIZE).repeat()\n",
    "\treturn dataset\n",
    "\n",
    "def get_eval_dataset():\n",
    "\tglob = 'gs://' + config.DATA_BUCKET + '/' + config.FOLDER + '/' + config.EVAL_BASE + '*'\n",
    "\tdataset = get_dataset(glob)\n",
    "\tdataset = dataset.batch(1).repeat()\n",
    "\treturn dataset\n",
    "\n",
    "# A variant of the UNET model.\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "\tencoder = layers.BatchNormalization()(encoder)\n",
    "\tencoder = layers.Activation('relu')(encoder)\n",
    "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "\tencoder = layers.BatchNormalization()(encoder)\n",
    "\tencoder = layers.Activation('relu')(encoder)\n",
    "\treturn encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "\tencoder = conv_block(input_tensor, num_filters)\n",
    "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "\treturn encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\treturn decoder\n",
    "\n",
    "def get_model():\n",
    "\tinputs = layers.Input(shape=[None, None, len(config.BANDS)]) # 256\n",
    "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
    "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
    "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
    "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
    "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
    "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
    "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
    "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
    "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
    "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
    "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
    "\toutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
    "\n",
    "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "\tmodel.compile(\n",
    "\t\toptimizer=optimizers.get(config.OPTIMIZER), \n",
    "\t\tloss=losses.get(config.LOSS),\n",
    "\t\tmetrics=[metrics.get(metric) for metric in config.METRICS])\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0F5czqrABgk"
   },
   "source": [
    "Verify that `model.py` is functioning as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8b0I9BaJ-GXw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 256, 256, 9), dtype=float32, numpy=\n",
      "array([[[[0.0276 , 0.0326 , 0.0582 , ..., 0.1083 , 0.2035 , 0.1895 ],\n",
      "         [0.0276 , 0.0326 , 0.0582 , ..., 0.1083 , 0.2035 , 0.1895 ],\n",
      "         [0.0314 , 0.0368 , 0.0679 , ..., 0.1172 , 0.2065 , 0.1905 ],\n",
      "         ...,\n",
      "         [0.0159 , 0.01885, 0.0391 , ..., 0.06775, 0.1965 , 0.1775 ],\n",
      "         [0.0159 , 0.01885, 0.0391 , ..., 0.06775, 0.1965 , 0.1775 ],\n",
      "         [0.01445, 0.01785, 0.0361 , ..., 0.0628 , 0.1965 , 0.1775 ]],\n",
      "\n",
      "        [[0.0263 , 0.0296 , 0.0537 , ..., 0.0917 , 0.2055 , 0.1885 ],\n",
      "         [0.0263 , 0.0296 , 0.0537 , ..., 0.0917 , 0.2055 , 0.1885 ],\n",
      "         [0.02835, 0.0326 , 0.0606 , ..., 0.10645, 0.209  , 0.1945 ],\n",
      "         ...,\n",
      "         [0.01605, 0.01965, 0.04185, ..., 0.0653 , 0.2015 , 0.1775 ],\n",
      "         [0.01605, 0.01965, 0.04185, ..., 0.0653 , 0.2015 , 0.1775 ],\n",
      "         [0.01585, 0.02025, 0.04225, ..., 0.0657 , 0.203  , 0.1815 ]],\n",
      "\n",
      "        [[0.0262 , 0.0311 , 0.0536 , ..., 0.0861 , 0.2085 , 0.1945 ],\n",
      "         [0.0262 , 0.0311 , 0.0536 , ..., 0.0861 , 0.2085 , 0.1945 ],\n",
      "         [0.0265 , 0.0296 , 0.0559 , ..., 0.094  , 0.2095 , 0.2005 ],\n",
      "         ...,\n",
      "         [0.0162 , 0.0206 , 0.0465 , ..., 0.06855, 0.2085 , 0.184  ],\n",
      "         [0.0162 , 0.0206 , 0.0465 , ..., 0.06855, 0.2085 , 0.184  ],\n",
      "         [0.0197 , 0.0256 , 0.0511 , ..., 0.08045, 0.21   , 0.1895 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157 , 0.0201 , 0.0379 , ..., 0.0509 , 0.1865 , 0.1725 ],\n",
      "         [0.02195, 0.02705, 0.0524 , ..., 0.0754 , 0.1965 , 0.179  ],\n",
      "         [0.02195, 0.02705, 0.0524 , ..., 0.0754 , 0.1965 , 0.179  ],\n",
      "         ...,\n",
      "         [0.0105 , 0.0132 , 0.0272 , ..., 0.0314 , 0.1955 , 0.1845 ],\n",
      "         [0.0105 , 0.01335, 0.02925, ..., 0.0346 , 0.195  , 0.1805 ],\n",
      "         [0.0105 , 0.01335, 0.02925, ..., 0.0346 , 0.195  , 0.1805 ]],\n",
      "\n",
      "        [[0.0173 , 0.0203 , 0.0401 , ..., 0.0545 , 0.1875 , 0.1735 ],\n",
      "         [0.02125, 0.02735, 0.05265, ..., 0.08935, 0.1965 , 0.1785 ],\n",
      "         [0.02125, 0.02735, 0.05265, ..., 0.08935, 0.1965 , 0.1785 ],\n",
      "         ...,\n",
      "         [0.0101 , 0.0123 , 0.0267 , ..., 0.0355 , 0.1965 , 0.1785 ],\n",
      "         [0.0099 , 0.0139 , 0.0303 , ..., 0.0366 , 0.1965 , 0.1785 ],\n",
      "         [0.0099 , 0.0139 , 0.0303 , ..., 0.0366 , 0.1965 , 0.1785 ]],\n",
      "\n",
      "        [[0.01655, 0.0197 , 0.03715, ..., 0.05075, 0.1875 , 0.172  ],\n",
      "         [0.01965, 0.02395, 0.0487 , ..., 0.07765, 0.196  , 0.178  ],\n",
      "         [0.01965, 0.02395, 0.0487 , ..., 0.07765, 0.196  , 0.178  ],\n",
      "         ...,\n",
      "         [0.011  , 0.01355, 0.0287 , ..., 0.0433 , 0.1985 , 0.1775 ],\n",
      "         [0.0107 , 0.0144 , 0.0265 , ..., 0.0398 , 0.1975 , 0.1795 ],\n",
      "         [0.0107 , 0.0144 , 0.0265 , ..., 0.0398 , 0.1975 , 0.1795 ]]]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 256, 256, 1), dtype=float32, numpy=\n",
      "array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]], dtype=float32)>)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 3 2624        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, None, 3 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, None, 3 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 3 9248        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, None, None, 3 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 6 18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 6 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 6 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 6 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 6 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 1 73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 1 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 1 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 1 147584      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 1 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 1 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 2 295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 2 1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 2 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 2 590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 2 1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 2 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 2 0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 5 1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 5 2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 5 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 5 2359808     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 5 2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 5 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 5 0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 1 4719616     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 1 4096        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 1 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 1 9438208     activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 1 4096        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 1 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, None, None, 5 2097664     activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 1 0           activation_9[0][0]               \n",
      "                                                                 conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 1 4096        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 1 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 5 4719104     activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 5 2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 5 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 5 2359808     activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 5 2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 5 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, None, None, 2 524544      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 5 0           activation_7[0][0]               \n",
      "                                                                 conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 5 2048        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None, 5 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 2 1179904     activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 2 1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, None, 2 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 2 590080      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 2 1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, None, 2 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, None, None, 1 131200      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 2 0           activation_5[0][0]               \n",
      "                                                                 conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 2 1024        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, None, 2 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 1 295040      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 1 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, None, 1 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 1 147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 1 512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, None, 1 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, None, None, 6 32832       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, None, 1 0           activation_3[0][0]               \n",
      "                                                                 conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 1 512         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 1 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 6 73792       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 6 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 6 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 6 36928       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 6 256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 6 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, None, None, 3 8224        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, None, None, 6 0           activation_1[0][0]               \n",
      "                                                                 conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 6 256         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 6 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 3 18464       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 3 128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 3 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 3 9248        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 3 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 3 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 1 33          activation_26[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 31,128,225\n",
      "Trainable params: 31,112,225\n",
      "Non-trainable params: 16,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from ai_platform_demo import model\n",
    "\n",
    "eval = model.get_eval_dataset()\n",
    "print(iter(eval.take(1)).next())\n",
    "\n",
    "model = model.get_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lul-C8DBXHT"
   },
   "source": [
    "## Training task\n",
    "\n",
    "At this stage, there should be `config.py` storing variables and `model.py` which has code for getting the training/evaluation data and the model.  All that's left is code for training the model.  The following will create `task.py`, which will get the training and eval data, train the model and save it when it's done in a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aR8GrYZd-Gb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ai_platform_demo/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PACKAGE_PATH}/task.py\n",
    "\n",
    "from . import config\n",
    "from . import model\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  training = model.get_training_dataset()\n",
    "  evaluation = model.get_eval_dataset()\n",
    "\n",
    "  m = model.get_model()\n",
    "\n",
    "  m.fit(\n",
    "      x=training,\n",
    "      epochs=config.EPOCHS, \n",
    "      steps_per_epoch=int(config.TRAIN_SIZE / config.BATCH_SIZE), \n",
    "      validation_data=evaluation,\n",
    "      validation_steps=int(config.EVAL_SIZE),\n",
    "      callbacks=[tf.keras.callbacks.TensorBoard(config.LOGS_DIR)])\n",
    "\n",
    "  m.save(config.MODEL_DIR, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTYQ8ftjCqgP"
   },
   "source": [
    "# Submit the package to AI Platform for training\n",
    "\n",
    "Now there's everything to submit this job, which can be done from the command line.  First, define some needed variables.\n",
    "\n",
    "**Note:** You need to insert the name of a Cloud project (below) you own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "p-PtuGdnEGcv"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "JOB_NAME = 'demo_training_job_' + str(int(time.time()))\n",
    "TRAINER_PACKAGE_PATH = 'ai_platform_demo'\n",
    "MAIN_TRAINER_MODULE = 'ai_platform_demo.task'\n",
    "REGION = 'northamerica-northeast1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVXri8QJETBb"
   },
   "source": [
    "Now the training job is ready to be started.  First, you need to enable the ML API for your project.  This can be done from [this link to the Cloud Console](https://console.developers.google.com/apis/library/ml.googleapis.com).  See [this guide](https://cloud.google.com/ml-engine/docs/tensorflow/training-jobs) for details.  Note that the Python and Tensorflow versions should match what is used in the Colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "B_sQ1mo6-Gef"
   },
   "outputs": [],
   "source": [
    "# !gcloud ai-platform jobs submit training {JOB_NAME} \\\n",
    "#     --job-dir {config.JOB_DIR}  \\\n",
    "#     --package-path {TRAINER_PACKAGE_PATH} \\\n",
    "#     --module-name {MAIN_TRAINER_MODULE} \\\n",
    "#     --region {REGION} \\\n",
    "#     --project {config.PROJECT} \\\n",
    "#     --runtime-version 2.7\\\n",
    "#     --python-version 3.7 \\\n",
    "#     --scale-tier custom \\\n",
    "#     --master-machine-type n1-highcpu-32 \n",
    "#     # --service-account '171083136856-compute@developer.gserviceaccount.com'\n",
    "#     #c\n",
    "#     # --service-account '171083136856@cloudservices.gserviceaccount.com'\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [demo_training_job_1642446370] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe demo_training_job_1642446370\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs demo_training_job_1642446370\n",
      "jobId: demo_training_job_1642446370\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training {JOB_NAME} \\\n",
    "    --job-dir {config.JOB_DIR}  \\\n",
    "    --package-path {TRAINER_PACKAGE_PATH} \\\n",
    "    --module-name {MAIN_TRAINER_MODULE} \\\n",
    "    --region {REGION} \\\n",
    "    --project {config.PROJECT} \\\n",
    "    --runtime-version 2.7\\\n",
    "    --python-version 3.7 \\\n",
    "    --scale-tier custom \\\n",
    "    --master-machine-type n1-highmem-32 \\\n",
    "    --master-accelerator count=2,type=NVIDIA-TESLA-P4 \n",
    "# --master-accelerator count=4,type=NVIDIA-TESLA-P4 \n",
    "    # --service-account '171083136856-compute@developer.gserviceaccount.com'\n",
    "    # --scale-tier basic-gpu\n",
    "    # --service-account '171083136856@cloudservices.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for adding GPU\n",
    "# !gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "#         --package-path $APP_PACKAGE_PATH \\\n",
    "#         --module-name $MAIN_APP_MODULE \\\n",
    "#         --job-dir $JOB_DIR \\\n",
    "#         --region us-central1 \\\n",
    "#         --scale-tier custom \\\n",
    "#         --master-machine-type n1-highcpu-16 \\\n",
    "#         --master-accelerator count=4,type=nvidia-tesla-t4 \\\n",
    "#         --worker-count 9 \\\n",
    "#         --worker-machine-type n1-highcpu-16 \\\n",
    "#         --worker-accelerator count=4,type=nvidia-tesla-t4 \\\n",
    "#         --parameter-server-count 3 \\\n",
    "#         --parameter-server-machine-type n1-highmem-8 \\\n",
    "#         -- \\\n",
    "#         --user_arg_1 value_1 \\\n",
    "#          ...\n",
    "#         --user_arg_n value_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6R65k0viIJS"
   },
   "source": [
    "## Monitor the training job\n",
    "\n",
    "There's not much more to do until the model is finished training (~24 hours), but it's fun and useful to monitor its progress. You can do that programmatically with another `gcloud` command.  The output of that command can be read into an `IPython.utils.text.SList` from which the `state` is extracted and ensured to be `SUCCEEDED`.  Or you can monitor it from the [AI Platform jobs page](http://console.cloud.google.com/ai-platform/jobs) on the Cloud Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1oqR6sCrEGoB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"createTime: '2022-01-17T19:06:49Z'\", \"endTime: '2022-01-17T20:29:09Z'\", 'etag: 2th8NM8ZeqU=', 'jobId: demo_training_job_1642446370', \"jobPosition: '0'\", \"startTime: '2022-01-17T19:17:31Z'\", 'state: SUCCEEDED', 'trainingInput:', '  jobDir: gs://eealr/fcnn-demo/trainer', '  masterConfig:', '    acceleratorConfig:', \"      count: '2'\", '      type: NVIDIA_TESLA_P4', '  masterType: n1-highmem-32', '  packageUris:', '  - gs://eealr/fcnn-demo/trainer/packages/2db3ad8913d7b8f8eb0e5f2941ad1b0c37168de65965e8a4a02cde3139c60c9a/ai_platform_demo-0.0.0.tar.gz', '  pythonModule: ai_platform_demo.task', \"  pythonVersion: '3.7'\", '  region: northamerica-northeast1', \"  runtimeVersion: '2.7'\", '  scaleTier: CUSTOM', 'trainingOutput:', '  consumedMLUnits: 7.31', '', 'View job in the Cloud Console at:', 'https://console.cloud.google.com/mlengine/jobs/demo_training_job_1642446370?project=ccmeo-ag-000008', '', 'View logs at:', 'https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Fdemo_training_job_1642446370&project=ccmeo-ag-000008']\n",
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'ccmeo-ag-000008'\n",
    "# print (PROJECT)\n",
    "desc = !gcloud ai-platform jobs describe {JOB_NAME} --project {PROJECT}\n",
    "print (desc)\n",
    "state = desc.grep('state:')[0].split(':')[1].strip()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFnIrvO0StiO"
   },
   "source": [
    "# Inspect the trained model\n",
    "\n",
    "Once the training job has finished, verify that you can load the trained model and print a summary of the fitted parameters.  It's also useful to examine the logs with [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard).  There's a convenient notebook extension that will launch TensorBoard in the Colab notebook.  Examine the training and testing learning curves to ensure that the training process has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "T9GU8Pl-2Y5p"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d54cfbd209c1e093\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d54cfbd209c1e093\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVRifmE2ffvv"
   },
   "source": [
    "# Prepare the model for making predictions in Earth Engine\n",
    "\n",
    "Before we can use the model in Earth Engine, it needs to be hosted by AI Platform.  But before we can host the model on AI Platform we need to *EEify* (a new word!) it.  The EEification process merely appends some extra operations to the input and outputs of the model in order to accommodate the interchange format between pixels from Earth Engine (float32) and inputs to AI Platform (base64).  (See [this doc](https://cloud.google.com/ml-engine/docs/online-predict#binary_data_in_prediction_input) for details.)  \n",
    "\n",
    "## `earthengine model prepare`\n",
    "The EEification process is handled for you using the Earth Engine command `earthengine model prepare`.  To use that command, we need to specify the input and output model directories and the name of the input and output nodes in the TensorFlow computation graph.  We can do all that programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KTzneSE_2WgL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'{\"serving_default_input_1:0\": \"array\"}'\n",
      "'{\"StatefulPartitionedCall:0\": \"impervious\"}'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "meta_graph_def = saved_model_utils.get_meta_graph_def(config.MODEL_DIR, 'serve')\n",
    "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
    "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
    "\n",
    "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
    "# model only has a single input and a single output.\n",
    "input_name = None\n",
    "for k,v in inputs.items():\n",
    "  input_name = v.name\n",
    "  break\n",
    "\n",
    "output_name = None\n",
    "for k,v in outputs.items():\n",
    "  output_name = v.name\n",
    "  break\n",
    "\n",
    "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
    "# AI Platform inputs and outputs, respectively.\n",
    "import json\n",
    "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
    "output_dict = \"'\" + json.dumps({output_name: \"impervious\"}) + \"'\"\n",
    "print(input_dict)\n",
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://eealr/fcnn-demo/trainer/model\n",
      "gs://eealr/fcnn-demo/trainer/eeified\n"
     ]
    }
   ],
   "source": [
    "print (config.MODEL_DIR)\n",
    "print (config.EEIFIED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "C1rA8oyscmGG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved project id\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1380, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1363, in _run_fn\n",
      "    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1456, in _call_tf_sessionrun\n",
      "    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) RESOURCE_EXHAUSTED: SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;0000000000000001;/job:localhost/replica:0/task:0/device:GPU:0;edge_658_StatefulPartitionedCall_2/RestoreV2;0:0\n",
      "\t [[{{function_node __inference__traced_restore_108529}}{{node RestoreV2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "\t [[GroupCrossDeviceControlEdges_0/StatefulPartitionedCall_2/Identity_172/_354]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "  (1) RESOURCE_EXHAUSTED: SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;0000000000000001;/job:localhost/replica:0/task:0/device:GPU:0;edge_658_StatefulPartitionedCall_2/RestoreV2;0:0\n",
      "\t [[{{function_node __inference__traced_restore_108529}}{{node RestoreV2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/bin/earthengine\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/eecli.py\", line 82, in main\n",
      "    tf_module.app.run(_run_command, argv=sys.argv[:1])\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/absl/app.py\", line 300, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/eecli.py\", line 63, in _run_command\n",
      "    dispatcher.run(args, config)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/commands.py\", line 360, in run\n",
      "    self.command_dict[vars(args)[self.dest]].run(args, config)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/commands.py\", line 360, in run\n",
      "    self.command_dict[vars(args)[self.dest]].run(args, config)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/commands.py\", line 1798, in run\n",
      "    new_model_dir = PrepareModelCommand._make_rpc_friendly(\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/ee/cli/commands.py\", line 1715, in _make_rpc_friendly\n",
      "    meta_graph = tf.saved_model.load(sesh, [tag], model_dir)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 348, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 346, in load\n",
      "    return loader.load(sess, tags, import_scope, **saver_kwargs)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 503, in load\n",
      "    self.restore_variables(sess, saver, import_scope)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 454, in restore_variables\n",
      "    saver.restore(sess, self._variables_path)\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 1404, in restore\n",
      "    sess.run(self.saver_def.restore_op_name,\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 970, in run\n",
      "    result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1193, in _run\n",
      "    results = self._do_run(handle, final_targets, final_fetches,\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1373, in _do_run\n",
      "    return self._do_call(_run_fn, feeds, fetches, targets, options,\n",
      "  File \"/home/gang_hong/anaconda3/envs/leaf/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1399, in _do_call\n",
      "    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) RESOURCE_EXHAUSTED: SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;0000000000000001;/job:localhost/replica:0/task:0/device:GPU:0;edge_658_StatefulPartitionedCall_2/RestoreV2;0:0\n",
      "\t [[{{node RestoreV2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "\t [[GroupCrossDeviceControlEdges_0/StatefulPartitionedCall_2/Identity_172/_354]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "  (1) RESOURCE_EXHAUSTED: SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;0000000000000001;/job:localhost/replica:0/task:0/device:GPU:0;edge_658_StatefulPartitionedCall_2/RestoreV2;0:0\n",
      "\t [[{{node RestoreV2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n"
     ]
    }
   ],
   "source": [
    "# You need to set the project before using the model prepare command.\n",
    "!earthengine set_project {PROJECT}\n",
    "!earthengine model prepare --source_dir {config.MODEL_DIR} --dest_dir {config.EEIFIED_DIR} --input {input_dict} --output {output_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buDXUtISnwm0"
   },
   "source": [
    "Note that you can also use the TensorFlow saved model command line tool to do this manually.  See [this doc](https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel) for details.  Also note the names we've specified for the new inputs and outputs: `array` and `impervious`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hno8QSo-2XjQ"
   },
   "source": [
    "# Perform inference using the trained model in Earth Engine\n",
    "\n",
    "Before it's possible to get predictions from the trained and EEified model, it needs to be deployed on AI Platform.  The first step is to create the model.  The second step is to create a version.  See [this guide](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models) for details.  Note that models and versions can be monitored from the [AI Platform models page](http://console.cloud.google.com/ai-platform/models) of the Cloud Console.\n",
    "\n",
    "To ensure that the model is ready for predictions without having to warm up nodes, you can use a configuration yaml file to set the scaling type of this version to `autoScaling`, and, set a minimum number of nodes for the version. This will ensure there are always nodes on stand-by, however, you will be charged as long as they are running. For this example, we'll set the `minNodes` to 10. That means that at a minimum, 10 nodes are always up and running and waiting for predictions. The number of nodes will also scale up automatically if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "6Wtt5NYZDjB7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "autoScaling:\n",
    "    minNodes: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "KSp34aCaySu5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating version: v1642359119\n",
      "Using endpoint [https://northamerica-northeast1-ml.googleapis.com/]\n",
      "Created ai platform model [projects/ccmeo-ag-000008/models/fcnn_demo_model].\n",
      "Using endpoint [https://northamerica-northeast1-ml.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.versions.create) FAILED_PRECONDITION: Field: version.deployment_uri Error: The provided URI for model files doesn't contain any objects.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: The provided URI for model files doesn't contain any objects.\n",
      "    field: version.deployment_uri\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'fcnn_demo_model'\n",
    "VERSION_NAME = 'v' + str(int(time.time()))\n",
    "print('Creating version: ' + VERSION_NAME)\n",
    "\n",
    "!gcloud ai-platform models create {MODEL_NAME} \\\n",
    "  --project {PROJECT} \\\n",
    "  --region {REGION}\n",
    "\n",
    "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
    "  --project {config.PROJECT} \\\n",
    "  --model {MODEL_NAME} \\\n",
    "  --region {REGION} \\\n",
    "  --origin {config.EEIFIED_DIR} \\\n",
    "  --framework \"TENSORFLOW\" \\\n",
    "  --runtime-version 2.4 \\\n",
    "  --python-version 3.7 \\\n",
    "  --config=config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4IfxvhmDWS"
   },
   "source": [
    "There is now a trained model, prepared for serving to Earth Engine, hosted and versioned on AI Platform.  We can now connect Earth Engine directly to the trained model for inference.  You do that with the `ee.Model.fromAiPlatformPredictor` command.\n",
    "\n",
    "## `ee.Model.fromAiPlatformPredictor`\n",
    "For this command to work, we need to know a lot about the model.  To connect to the model, you need to know the name and version.\n",
    "\n",
    "### Inputs\n",
    "You need to be able to recreate the imagery on which it was trained in order to perform inference.  Specifically, you need to create an array-valued input from the scaled data and use that for input.  (Recall that the new input node is named `array`, which is convenient because the array image has one band, named `array` by default.)  The inputs will be provided as 144x144 patches (`inputTileSize`), at 30-meter resolution (`proj`), but 8 pixels will be thrown out (`inputOverlapSize`) to minimize boundary effects.\n",
    "\n",
    "### Outputs\n",
    "The output (which you also need to know), is a single float band named `impervious`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqcgSGvx-E94"
   },
   "outputs": [],
   "source": [
    "# Use Landsat 8 surface reflectance data.\n",
    "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "\n",
    "# Cloud masking function.\n",
    "def maskL8sr(image):\n",
    "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
    "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
    "  qa = image.select('pixel_qa')\n",
    "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
    "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
    "  mask2 = image.mask().reduce('min')\n",
    "  mask3 = image.select(config.opticalBands).gt(0).And(\n",
    "          image.select(config.opticalBands).lt(10000)).reduce('min')\n",
    "  mask = mask1.And(mask2).And(mask3)\n",
    "  return image.select(config.opticalBands).divide(10000).addBands(\n",
    "          image.select(config.thermalBands).divide(10).clamp(273.15, 373.15)\n",
    "            .subtract(273.15).divide(100)).updateMask(mask)\n",
    "\n",
    "# The image input data is a cloud-masked median composite.\n",
    "image = l8sr.filterDate(\n",
    "    '2015-01-01', '2017-12-31').map(maskL8sr).median().select(config.BANDS).float()\n",
    "\n",
    "# Load the trained model and use it for prediction.  If you specified a region \n",
    "# other than the default (us-central1) at model creation, specify it here.\n",
    "model = ee.Model.fromAiPlatformPredictor(\n",
    "    projectName = config.PROJECT,\n",
    "    modelName = MODEL_NAME,\n",
    "    version = VERSION_NAME,\n",
    "    inputTileSize = [144, 144],\n",
    "    inputOverlapSize = [8, 8],\n",
    "    proj = ee.Projection('EPSG:4326').atScale(30),\n",
    "    fixInputProj = True,\n",
    "    outputBands = {'impervious': {\n",
    "        'type': ee.PixelType.float()\n",
    "      }\n",
    "    }\n",
    ")\n",
    "predictions = model.predictImage(image.toArray())\n",
    "\n",
    "# Use folium to visualize the input imagery and the predictions.\n",
    "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
    "map = folium.Map(location=[38., -122.5], zoom_start=13)\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(map)\n",
    "\n",
    "mapid = predictions.getMapId({'min': 0, 'max': 1})\n",
    "\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='predictions',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI_platform_demo.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
